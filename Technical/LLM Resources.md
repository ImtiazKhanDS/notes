---
id: LLM Resources
aliases: []
tags: []
share: true
---
1. Watch video by Andrew karpathy -- done  
2. Watch video by Jeremy howard  -- done
3. Explore langchain   --In-progress
4. Explore Cody neovim  -- done
5. Explore llama2  -- In-progress
6. Explore mixtral  In-progress
7. Explore mamba architecture  
8. Explore Rag based approaches -- In-progress 
9. Search  
10. Ranking  
11. Personalisation of search results.  
12. survey of large language models : https://arxiv.org/pdf/2402.06196.pdf
13. The Making of SRKGPT: Crafting an AI with Shahrukh Khan's Style | LLM | LLama v2 Finetuning youtu.be/gYPwx0DR7zc 
14. Supercharging LLama-2: Enhancing Performance on Any Task with ChatGPT Dataset youtu.be/paGr-t1wSOQ
15. Elevating Base Falcon Model with ChatGPT Dataset: A Game-Changing Approach youtu.be/lo11Iczb0Vc
17. LLAMA-2 Open-Source LLM: Custom Fine-tuning Made Easy on a Single-GPU Colab Instance | PEFT | LORA youtu.be/8cc4bJtycOA
18. Falcon Open-Source LLM: Custom Fine-tuning Made Easy on a Single-GPU Colab Instance | PEFT | LORA youtu.be/CxqZ5j3xlt0
19. Conversational AI : Understanding the Technology behind Chat-GPT | GPT| RLHF | Few Shot Inferences youtu.be/JKoJ5YIr2O4
20.  The Good, the Bad, & the Ugly : Exploring the Dual Nature of Conversational LLMs | Open-Source LLMs youtu.be/MHfzoHC4kek
21. Pushing the Boundaries of LLMs: Sparse & Flash Attention, Quantisation, Pruning, Distillation, LORA youtu.be/mF7OM_XU2S4
  
21. Kaggle's LLM Science Exam competition [https://lnkd.in/gvrFhfR8](https://lnkd.in/gvrFhfR8) made participants answer hard science questions. The winning solution showed Llama-2 70b with prompting gets 80%. + finetuning via SFT you get 86%. But + finetuning + RAG you get 93%. All had to undergo finetuning since the output was MMLU's classification type ie output A, B, C, D etc (so a classification problem).  
22. https://maartengrootendorst.substack.com/p/a-visual-guide-to-mamba-and-state
  
1. Together.ai all models in one place  
2. Colm website for research ideas
