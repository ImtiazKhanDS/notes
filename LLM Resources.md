---
id: LLM Resources
aliases: []
tags: []
share: true
---
1. Watch video by Andrew karpathy -- done  
2. Watch video by Jeremy howard  -- done
3. Explore langchain   --In-progress
4. Explore Cody neovim  
5. Explore llama2  -- In-progress
6. Explore mixtral  In-progress
7. Explore mamba architecture  
8. Explore Rag based approaches -- In-progress  
9. Search  
10. Ranking  
11. Personalisation of search results.  
  
Kaggle's LLM Science Exam competition [https://lnkd.in/gvrFhfR8](https://lnkd.in/gvrFhfR8) made participants answer hard science questions. The winning solution showed Llama-2 70b with prompting gets 80%. + finetuning via SFT you get 86%. But + finetuning + RAG you get 93%. All had to undergo finetuning since the output was MMLU's classification type ie output A, B, C, D etc (so a classification problem).  
  
1. Together.ai all models in one place  
2. Colm website for research ideas
